## Portfolio

---

### Publications 

[Semantic Terrain Classification for Off-Road Autonomous Driving](https://openreview.net/forum?id=AL4FPs84YdQ)
<img src="images/warthog.png?raw=true"/>
<img src="images/canal.gif?raw=true"/>
<br></br>
<b>
Accepted Conference on Robot Learning (CoRL) 2021
</b>
<p>
Abstract: Producing dense and accurate traversability maps is crucial for autonomous off-road navigation. In this paper, we focus on the problem of classifying terrains into 4 cost classes (free, low-cost, medium-cost, obstacle) for traversability assessment. This requires a robot to reason about both semantics (what objects are present?) and geometric properties (where are the objects located?) of the environment. To achieve this goal, we develop a novel Bird's Eye View Network (BEVNet), a deep neural network that directly predicts a local map encoding terrain classes from sparse LiDAR inputs. BEVNet processes both geometric and semantic information in a temporally consistent fashion. More importantly, it uses learned prior and history to predict terrain classes in unseen space and into the future, allowing a robot to better appraise its situation. We quantitatively evaluate BEVNet on both on-road and off-road scenarios and show that it outperforms a variety of strong baselines.
</p>

### Research
[Into the Wild: Robust Offroad Driving with Deep Perception](/pdf/JoonHo_thesis.pdf)
<img src="images/canal.png?raw=true"/>
<br></br>
<b>
Undergraduate Honors Thesis
</b>
<p>
Abstract: The task of autonomous offroad driving yields great potential for various beneficial applications, including but not limited to remote disaster relief, environment survey, and agricultural robotics. While achieving the task of robust offroad driving poses relatively new, interesting challenges to tackle, the most important requirement for a successful offroad autonomy is observed to be an effective understanding of the vehicle surrounding for robust navigation and driving. Therefore, in this thesis we tackle the task of scene understanding for autonomous offroad driving. We formulate the task of scene understanding as a traversability classification task, and develop a multimodal perception framework that extracts semantic knowledge. As our key contribution we propose a multimodal perception framework that uses convolutional neural networks with image and LiDAR input. The pipeline generates semantic knowledge from input data for robust mapping, planning, and control in the wild environment. We evaluate our method by integrating it into an autonomy stack and demonstrating its performance in a set of environments under various weather conditions.
</p>

<b>Real robot demo</b>

---

[comment]: <> ([Project 3 Title]&#40;http://example.com/&#41;)

[comment]: <> (<img src="images/dummy_thumbnail.jpg?raw=true"/>)

[comment]: <> (---)

[comment]: <> (### Category Name 2)

[comment]: <> (- [Project 1 Title]&#40;http://example.com/&#41;)

[comment]: <> (- [Project 2 Title]&#40;http://example.com/&#41;)

[comment]: <> (- [Project 3 Title]&#40;http://example.com/&#41;)

[comment]: <> (- [Project 4 Title]&#40;http://example.com/&#41;)

[comment]: <> (- [Project 5 Title]&#40;http://example.com/&#41;)

[comment]: <> (---)




---
<p style="font-size:11px">Page template forked from <a href="https://github.com/evanca/quick-portfolio">evanca</a></p>
<!-- Remove above link if you don't want to attibute -->
