## Florence: A New Foundation Model for Computer Vision
[[Paper]](https://arxiv.org/abs/2111.11432)

* Florence is UniCL ran at much larger scale in hopes of developing a stronger foundation model

* First, Authors curate a 900M dataset mixture of publicly available images and generate (image, text, label) triplets for UniCL based pre-training. Labels are generated by finding common concepts based on the acquire text descriptions.

* Pre-training is done in 512 with A-100 (40GB) GPUs over 10 days

* As a data augmentation technique authors use prompt templates e.g. "A photo of the [[WORD]]".

* 1st stage pre-training: 224 x 224 image size with batch size ~24K, sequence length truncated to 70 approx. (language prompts included in dta)
* 2nd stage pre-training: 384x384 resolution (language prompts removed)

* Once they have pre-trained the model, the authors integrated various prior work on decoder/head networks architectures for various Vision and Vision+Language tasks, ultimately achieving new SOTA on over 44 benchmarks across recognition, image (video) retrival, object detection, VQA, and more.

* Florence is available through Azure cognitive services

## Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks

[[ArXiv]](https://arxiv.org/pdf/2311.06242.pdf)

* Extends Florence Paradigm by training a foundational model that extends from the original Florence model that now handles a variety of vision and vision-language tasks

* Takes in text-prompt as input and also outputs text-prompt, but it is multimodal --> image is processed by separate ViT encoder that produces set of patch embedding tokens to concatenate alongside the multi-task text prompts to the main transformer encoder-decoder network

* Developed a multi-modal, multi-task dataset titled FLD-5B to train the new model. FLD-5B was created with a data engine that iteraively collects data, finetunes model, and generates updated pseudo-labels as annotation. A set of supervised data sets have been used to train the initial model(s).

* above data engine process in essence -> 1) initially used trained specialist models per task 2) collected data is filtered and refined to train Florence-2, with 3) step 2 iteratively repeated to end up with final Florence-2 model and labels (that would essentially allow reproducing florence-2 without the iterative refinement and intial data collection)

* Specifica special tokens were added to its vocabulary to ensure valid output format for the vision tasks - 1000 quantized tokens representing different coordinates in the normalized image coordinates allows the model to fluidly predict output for various vision-related tasks.