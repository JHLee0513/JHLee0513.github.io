# Image and LiDAR Self-Supervised Learning: A Survey

Perception tasks for automatic robotic systems, including but not limited to 2D/3D object detection, pose estimation, semantic segmetation, and BEV segmentation, has shown impressive progress with Deep Learning based Methods. Neural Networks use input from various sensors such as RGB(D) camera, LiDAR, Radar to perform such tasks. In particular, the field is yet to see an increase in the interest in two related fields - self supervised learning and multi-modal fusion.

**Self Supervised Learing**


As it is in my interest to research towards scalable multimodal fusion based based perception with self supervised learning, this blog focuses on those within the context of autonomous driving, self supervised learning, and multi-modal fusion. In particular, we focus on BEV perception, where commonly approached tasks are 3D object detection and BEV (semantic map) segmentation.

## Background on Supervised BEV Perception

There has been a lot of progress in . To the best of my knowledge, this section covers interesting and/or notable prior work on BEV perception.

### Percetion to planning
1. End-to-End Interpretable Motion Planner
2. Perceive, Predict, and Plan
3. Perceive, Attend, and Drive: Learning Spatial Attention for Safe Self-Driving
4. MP3:

The corresponding section explores the line of work researched at the University of Toronto, namely Professor Raqel Urtasun who was formerly in Uber ATG and now founder of Waabi.


## SWaV

## Barlow Twins

## PointContrast

## P4Contrast

## CrossPoint: Self-Supervised Cross-Modal Contrastive Leraning for 3D Point Cloud Understanding (CVPR 2022)

## Contrastive Learning for Self-Supervised Pre-Training of Point Cloud Segmentation Networks With Image Data (Arxiv, 2023)
<a href="https://arxiv.org/abs/2301.07283">paper</a>

